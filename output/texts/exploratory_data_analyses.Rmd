---
title: "Exploratory data analyses and data preparation report - Knotweed Tarping Survey (Martin *et al.*, 2023)"
author: "François-Marie Martin"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_document:
     number_sections: yes
     toc: yes
     toc_depth: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# - Introduction
## - Regarding reproductibility

In order to facilitate any attempt at reproducing this study, here is a depiction of the system information used to prepare and analyse the data from the survey on tarping operations for the control of Japanese knotweed *s.l.* (*Reynoutria spp.*).  

```{r session information}
rm(list=ls())
sessionInfo()
```

All data and code used in this study can furthermore be found here: https://github.com/mrelnoob/jk.fanf.tarping
\
\

## - What is this document about?  

```{r sourcing the project functions, include=FALSE}
source(file = here::here("R/01_03_exploration_functions.R")) # I need to source my previous scripts (R files)
# to be able to import my cleaned dataset (with my 'model_datasets()' function) and perform my analyses.
.pardefault <- par() # To save the default graphical parameters (in case I want to restore them).
```
\
In our efforts to build valid and meaningful models to identify some of the drivers potentially influencing the success or failure of tarping operations for the control of Japanese knotweed *s.l.*, we had to learn more about our data.  
To explain the variations of our **four response variables** (i.e. *effectiveness*, *eradication*, *reg_edges*, and *reg_overlaps*), we created four slightly different sub-datasets from our global dataset. These sub-datasets differed in their *sample size* and in the *potential explanatory variables* they contained (because of our working hypotheses). They varied in their sample size because of the varying numbers of missing values in their associated response variables (i.e. NAs), and they varied in the potential explanatory variables they contained because we did not hypothesized that all response variables would be explained by the same set of explanatory variables. The present documents presents, for **each one** of the four response variables, the results of the **exploratory data analyses** and basic **assumption checks** performed on their respective sub-datasets.  
Various aspects of our data (e.g. outliers, variables distribution, multicollinearity among predictors, homogeneity of variances, independence, potential interactions, multivariate relationships, linearity etc.) were explored in this document, mostly following Bolker (2007), Zuur *et al.* (2010), and Harrell (2015). However, as the response variables were of different nature (e.g. binary, continuous), they would be modelled using different types of models with varying assumptions:

* The *effectiveness* and *eradication* variables shared the same sub-dataset because we decided to analyse them using the same set of candidate *a priori* models. The former variable is a sort of *satisfaction score* estimated by the managers and can be modelled as a percentage using **beta regressions** while the latter, which relates to cases of knotweed (near-)eradication, is a binary variable that should be modelled using some type of **logistic regression**.
* The two remaining response variables indicated the presence of knotweed regrowths at various key locations of the tarping set-ups, and should thus also be modelled using methods of the *logistic regression family*.

Naturally, most actual models assumptions were checked after fitting the models. Our intention here was simply to improve our knowledge of the data and identify potentially problematic variables upstream. As our global dataset contained many potential predictors (*p* = 85), much effort was indeed directed toward reducing their number and keeping only the most reliable and important ones relative to our hypotheses. In other words, we tried to reduce the dimensionality of the data.  
It should be noted that our study is **exploratory**. That means that our goal is not to make formal inference but to *explore* potential relationships in order to generate new hypotheses (that could then be formally tested in a rigourous inferential framework). Therefore, it also means that we can slacken some modelling assumptions and that we can look at the relationships between the *response variables* and their potential *predictors* (which is absolutely forbidden when the purpose of a study is inferential). In other words, we can take some liberty because we are trying to identify relationships that should be properly tested with new data (instead of claiming that we found robust "causal" links).

For the meaning of all variables, the reader is referred to Appendix §§§ *** §§§ !!!!!!
\
\

******

******

# - Exploration of the sub-dataset related to tarping *effectiveness*
\
The first thing to do here was to prepare the sub-dataset that will be used to model the responses of the *effectiveness variables* (i.e. **effectiveness** and **eradication**). To do so, the following steps have been performed:

* We imported the subset of predictors and covariates that we wanted to use to model tarping effectiveness (based on our knowledge and hypotheses).
* We left out observations that were too recent (tarping operations installed less than 2 growing seasons ago) and those for which we could not obtain an *effectiveness score* (i.e. NA).
* We deleted operations that got prematurely interrupted (e.g. because of vandalism) since their failure resulted from external causes.
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variable (computed as the Normalized Root Mean Squared Error (NRMSE) for numeric variables, and the Proportion of Falsely Classified entries (PFC) for categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation errors. For the meaning of the variables, please refer to the attached **documentation**.

```{r erad import & imput, include=FALSE}
erad <- model_datasets(response.var = "effectiveness")

erad %>% dplyr::select(-liner_geomem, -agri_geomem,-woven_geotex, -mulching_geotex, -pla_geotex, -other_unknown, -grammage, -age, -thickness, -pierced_tarpinstall, -add_control_type) %>%
  dplyr::filter(eff_eradication != "NA") %>%
  dplyr::filter(tarping_duration >= 2) %>%
  dplyr::filter(xp_id != "54") -> erad # Operation n°54 was destroyed because of vandalism.

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
erad %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> erad_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = erad_mis, verbose = TRUE)

# To create a small summary table for the OOB errors (for each variable, with the argument `variablewise = TRUE` in missForest):
# imput_error <- data.frame(cbind(
#   sapply(erad_mis, function(y) sum(length(which(is.na(y))))), # Number of imputed values (NAs)
#   imput$OOBerror), # Out-of-bag error (OOB) for each variable
#   row.names = colnames(erad_mis)) # To get the name of the variables
# dplyr::rename(imput_error, nb_imputed_values = 'X1', oob_error = 'X2') -> imput_error
```

```{r erad imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Here are a few descriptive statistics of our sub-dataset.

```{r erad summary, include=TRUE, message=FALSE, comment=NA}
erad[,3:ncol(erad)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(erad)

rm(erad_mis, imput)
```
\
\

## - Synthesis of contextual variables
\
We performed a **normed-PCA** on some of our environmental variables for which we did not have strong direct hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r erad env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 2.1.** Correlation plot of the normed-PCA on environmental variables for the *tarping effectiveness* dataset"}
xxx <- dplyr::select(.data = erad, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable colouration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> erad
erad$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of the PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of these 5 environmental variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable was named **coarse_env** and replaced the other 5 variables in the dataset. Operations with *positive values* for this variable were located in sites that tended to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

We also performed another **normed-PCA** on some of our follow-up variables that we thought could be synthesized (i.e. *add_control*, *repairs*, and *freq_monitoring*).


```{r erad followups, warning=FALSE, fig.align='center', fig.cap="**Figure 2.1b.** Correlation plot of the normed-PCA on follow-up variables for the *tarping effectiveness* dataset"}
erad %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
erad %>% dplyr::rename(followups = "freq_monitoring") -> erad
erad$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of the PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their follow-up conditions. This new variable was named **followups** and thus replaced *freq_monitoring* in the dataset (we decided to keep *repairs* and *add_control* as is). High *positive values* for this variable represent operations that were frequently monitored and where additional controls and repairs were performed (and vice-versa).
\
\

## - Univariate distribution and dispersion parameters
### - Looking for outliers
\
The next step in our data exploration was to look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r erad boxplot outliers, fig.align='center', fig.cap="**Figure 2.2.** Boxplots for all quantitative variables of the *eradication effectiveness* dataset", warning=FALSE}
rm(res.pca, xxx, ttt)

uni.boxplots(dataset = erad)
```

```{r erad dotplot, fig.align='center', fig.cap="**Figure 2.3.** Cleveland dotplots for all quantitative variables of the *eradication effectiveness* dataset"}
uni.dotplots(dataset = erad)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *latitude*, *slope* and *coarse_env* variables look fine.
* The *effectiveness*, *obstacles*, *flood*, *distance*, *tarping_duration*, *trench_depth*, and *followups* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *elevation* and *sedicover_height* variables may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *longitude* and *stand_surface* variables contain problematic patterns/outliers.


```{r erad simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = erad, elevation, stand_surface, distance, sedicover_height, trench_depth, tarping_duration)

uni.simudistrib(simu.var = simu.var, distribution = "normal")
uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)

```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we chose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not seem to have values that are too extreme to be possibly drawn from Normal, log-Normal or Poisson distributed variables, except *longitude*. Yet, we do not intend on truly using this variable at this point (we included it in case we need to account for some spatial autocorrelation in the models).


```{r erad update, include=FALSE}
erad <- erad[-(which.max(erad$stand_surface)),]
```
\
\
   
### - Normality
\
Although we already gained pretty good insights into the probability distribution followed by our variables, it is usually interesting and often useful to have a precise view of the variables' distributions.\
Consequently, we described the distributional shapes of our quantitative variables using histograms as well as skewness and kurtosis values. Skewness is a measure of the symmetry of a variable's distribution (a perfectly normal distribution has a skewness of 0), while kurtosis gives information on the combined amount of probability contained in the two tails of a distribution (often reported as *excess kurtosis*, that is kurtosis - 3, since a perfectly normally distributed variable has a kurtosis of 3). There is much debate regarding what constitutes "acceptable" values for these statistics or whether this question actually makes any sense at all. Yet, various authors suggest that values of ±2 should not be too problematic for procedures requiring approximately normally distributed variables (Field, 2009; Gravetter & Wallnau, 2014). Nonetheless, many statistical methods do not possess a direct normality assumption (linear regression models, for instance and contrarily to popular beliefs, do not assume the normality of either the response variable nor of any predictor/covariate but do assume that the replicated observations of the response variable for any fixed predictor value should be normally distributed (Montgomery & Peck, 1992); an assumption that is often difficult to verify, hence the usual focus on models residuals).
For all these reasons, here, we only used these statistics as descriptive tools to foresee potential future complications related to the distribution of our variables, but not as bases for actual decision making. 

```{r erad normality, fig.align='center', fig.cap="**Figure 2.4.** Histograms for all quantitative variables of the *eradication effectiveness* dataset"}
uni.histograms(dataset = erad)

num.data <- erad[, sapply(erad, is.numeric)]
tab <- data.frame(moments::skewness(x = num.data), moments::kurtosis(x = num.data)-3)
knitr::kable(x = tab, digits = 3, col.names = c("Skewness", "Excess kurtosis"))
rm(num.data, tab)
```

From the above histograms and table, we can see that:

* Very few of our sampled variables are close to approximate a normal distribution: perhaps *latitude*, *coarse_env*, and *followups*. Surprisingly, *effectiveness* possesses "acceptable" skewness and kurtosis values yet is clearly far from a normal variable (left-dissymmetric and bounded), perhaps attesting the limited usefulness of these statistics.
* Most variables are skewed to the right but in relatively reasonable proportions. 
* Some variables (e.g. *elevation*, *stand_surface*, *distance*, *trench_depth* and *sedicover_height*) have skewness and/or kurtosis values possibly high enough to have undue influence on regression coefficients. These variables will perhaps require some transformations.
\
\

## - Multivariate relationships
### - Correlation among variables and multicollinearity
\
To investigate potential correlations and collinearity among our variables, we started by computing a **correlation matrix** of the data. As most of our factors were either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we could use them in a correlation matrix. To do that however, computations were based on *Spearman*'s rank correlation coefficients.  
To avoid giving too much weight to tarping experiments made by the same operators (that could be autocorrelated), we removed them from the data to compute correlations: their influence was accounted for in further analyses (only when modelling diagnostics indicated autocorrelation) by the use of **mixed-effect models**.

```{r erad corrplot, fig.align='center', fig.cap="**Figure 2.5.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *tarping effectiveness* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}

# To convert all ordered factors into numeric variables usable in a correlation matrix:
erad %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id, -goals) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.erad # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.erad <- round(stats::cor(num.erad, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.erad <- ggcorrplot::cor_pmat(x = num.erad, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.erad, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.erad, insig = "blank")

rm(num.erad, res.cor.erad, res.pcor.erad)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* One or several of the *effectiveness* variables seem to be mildly positively correlated with *distance*, and *fully_tarped* and negatively with *obstacles*, *followups*, and *pb_durability*. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. *geotex* is also logically correlated with *woven_geotex*.
* Naturally, *followups* is strongly positively correlated with *repairs* and *add_control*. These latter two variables are also quite correlated with the variables related to *degradations*.
* *trench_depth* seems to be strongly positively correlated with *tarpfix_multimethod*, which is logical as trenches are strong fixation method. 
* *pb_fixation* seems to be strongly positively correlated with *stand_surface* and, to a lesser extent, with *tarpfix_pierced*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability*, and negatively with *distance*: all these correlations seem quite straightforward. 
* Interestingly, *flood* is positively correlated with *geotex*, *tarpfix_pierced*, and *pb_fixation*.

At this stage and in accordance with the hierarchy of hypotheses we would like to test for the *effectiveness* variables, we decided to drop the following variables: *woven_geotex*, *geotex*, and *degradations*.

```{r erad corrdrop}
erad %>% dplyr::select(-geotex, -weedsp_geotex, -degradation) -> erad
```

As multicollinearity is one of the biggest issues for modelling, assessing which explanatory variables and covariates are collinear and could possibly be problematic during modelling stages is important: for instance by computing *Variance Inflation Factors* (VIF) for each variable. 
However, as only a small fraction of the variables will actually be used together in the likely *a priori* models that we will build, we intend on computing VIF as part of the models diagnostic checks and not here. Furthermore, as several variables were categorical and as we will not be working with models fitted with ordinary least squares, we will use *Generalized VIF* instead of VIF (Fox & Monette, 1992).
\
\

### - Relationships among explanatory variables
\
To further study distributions and bivariate correlations, we decided to plot every numeric predictor against each others. It gives us the possibility to start investigating potential **multivariate outliers**.

```{r erad ggpairs, fig.align='center', fig.cap="**Figure 2.6.** Bivariate relationship among all numeric potential explanatory variables", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}

GGally::ggpairs(erad[, c("coarse_env", "distance", "flood", "followups", "elevation", "obstacles", 
                         "stand_surface", "sedicover_height", "slope", "tarping_duration")])
```

This figure confirms that most variables are far from being normally distributed but that's not necessarily a problem. If many scatterplots appear relatively compact and consistent, all highly skewed variables (e.g. *stand_surface*, *sedicover_height*, etc.) tend to produce what seem to be multivariate outliers that could become potentially problematic for modelling. In case it happens, transforming these variables may be useful.
\
\

### - Relationships between *effectiveness* and its potential explanatory variables
\
The next step in our data exploration was to plot the response variables against each predictors and covariates in order to assess the **linearity** and **homoscedasticity** of these relationships and help identify potential **multivariate outliers**. 

```{r erad bivariate plot, fig.align='center', fig.cap="**Figure 2.7.** Multivariate representation of the *effectiveness score* against each potential explanatory variable (note that for some unknown reason, the loess smoother failed to be drawn in some panels with binary predictors)", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}

### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "effectiveness" variable (bounded continuous variable):
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -eradication, -latitude, -longitude) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(effectiveness), # Means that all columns of `mydata` need to be reshaped except "effectiveness" (works as well with the form `!effectiveness`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = effectiveness)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,10)) +
  ggplot2::geom_smooth(method = "loess", na.rm = TRUE, size = 1, col = "plum1") + # If you want to affect something related to the input dataframe, you have to put the command in an aes() function (e.g. if we want a smoother for each value of "fully_tarped", we should have wrote ggplot2::aes(col = as.factor(fully_tarped)))! 
# If a "loess" smoother is used, sometimes the smoother line won't be plotted, I don't know why.
  ggplot2::scale_colour_manual(values = c("plum1", "palevioletred4")) +
  ggplot2::labs(x = "Predictor value", y = "effectiveness score", col = "Fully tarped") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Various interesting observations can be made from these scatterplots:

* First, heteroscedasticity will perhaps not be surprising in our models as the variances of many variables do not seem to be homogeneous. Yet, because the variance is a function of the mean in the beta distribution, beta regressions are naturally heteroscedastics so it may not be a problem here. 
* Second, we do not see clear bivariate outliers.
* Third, the **linearity assumption** may well be violated for several variables (e.g. *distance*, *sedicover_height*, *stand_surface*, *tarping_duration*). These variables will possibly require some transformation to be incorporated in beta models.
\
\

### - Relationships between *eradication* and its potential explanatory variables
\
Since *eradication* will be modelled using logistic regressions, it is very important to ensure the respect of the **linearity assumption** between the outcome on the *logit scale* and each numeric predictor, for instance using Box-Tidwell tests or scatterplots. The main issue here is to get the values of the response variable on the logit scale. To do so, we have to build a logistic model incorporating all potential predictors to extract expected probabilities. A drawback from this approach is that the toy model can easily be misspecified in the process: for that reason, we actually checked this assumption iteratively for each *a priori* candidate model built for analyses (the scatterplots displayed below only serve to illustrate what transformations were actually applied to linearize relationships).

```{r erad linearity scatterplot, fig.align='center', fig.cap="**Figure 2.8.** Bivariate scatterplot between the *eradication* variable on the *logit scale* and each potential numeric predictor", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
eff2 <- erad[,c("eradication", "distance", "followups", "obstacles", "sedicover_height", "slope", "stand_surface", "tarping_duration")]
model <- glm(eradication ~., data = eff2, family = binomial)
# Predict the probability (p) of eradication:
probabilities <- predict(model, type = "response")
# Transforming predictors
mydata <- eff2 %>%
  dplyr::select_if(is.numeric) %>%
  dplyr::mutate("stand_surface (log)" = log(stand_surface)) %>%
  dplyr::mutate("distance (log+1)" = log(distance + 1)) %>%
  dplyr::mutate("sedicover_height (log+1)" = log(sedicover_height+1)) # These transformations were made to linearize the relationships
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot (ggplot2, so long format)
mydata <- mydata %>%
  dplyr::mutate(logit = log(probabilities/(1-probabilities))) %>%
  tidyr::gather(key = "predictors", value = "predictor.value", -logit)
# Create scatterplot
ggplot2::ggplot(mydata, ggplot2::aes(y = logit, x = predictor.value))+
  ggplot2::geom_point(size = 0.5, alpha = 0.5) +
  ggplot2::geom_smooth(method = "loess") +
  ggplot2::theme_bw() +
  ggplot2::facet_wrap(~predictors, scales = "free_x")
rm(eff2, model, probabilities, predictors, mydata)
```

This figure shows that log-transforming *distance*, *sedicover_height* and *stand_surface* acceptably linearized their relationship with *eradication* expressed on the logit scale.

More general scatterplots can further be used to ensure the absence of bivariate **(quasi-)perfect separation** between a predictor and the response variable, which is highly problematic for logistic models fitted using Maximum Likelihood (Allison, 2004).

```{r erad bivariate plot2, fig.align='center', fig.cap="**Figure 2.9.** Multivariate representation of the *eradication* variable against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-manager_id, -xp_id, -goals, -eff_eradication, -effectiveness, -latitude, -longitude) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(eradication), 
                               names_to = "Predictor", 
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = eradication)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "coral") +
  #ggplot2::scale_y_continuous(limits=c(0,10.5)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "darkred") + 
  ggplot2::scale_colour_manual(values = c("coral", "darkred")) +
  ggplot2::labs(x = "Predictor value", y = "Eradication", col = "Plantation") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

Although most relationships look fine, we can see from these scatterplots that there is a clear case of **perfect separation** between *eradication* and *fully_tarped*! We will have to model this response variable using a penalised regression method, such as LASSO or Ridge regressions.
\
\
   
### - Homogeneity of variances

As stated before, **heteroscedasticity** is normally not an issue for *beta regression* models. Nonetheless, it is always useful to have a clear idea of the distribution of variances, so we decided to draw conditional boxplots of our only "continuous" response variable (i.e. *effectiveness*).

```{r erad cond.boxplots, fig.align='center', fig.cap="**Figure 2.10.** Bivariate conditional boxplots of the *effectiveness score* against each potential explanatory variable", fig.width=9, fig.height=7, message=FALSE, comment=NA, warning=FALSE}
erad %>% dplyr::select(-goals, -latitude, -longitude) -> erad1
cond.boxplots(dataset = erad1, Y = 3, Xs = 6:ncol(erad1), outlier = TRUE)
rm(erad1)
```

According to Fox (2008), for simple linear regression models, problematic heterogeneity of variance occurs when the ratio between the largest and smallest variance is 4 or more. Consequently, there is a possibility for the following variables to become problematic for modelling: *elevation*, *flood*, *stand_surface*, *tarping_duration*, *sedicover_height*, and *trench_depth*. Still, actual problematic patterns will be better observable by plotting residuals against model fitted values.
As *eradication* is binomial, **heteroscedasticity** is not a problem as variance is assumed to be heterogeneous in logistic regression models.
\
\
   
### - Interactions among variables

Finally, to further investigate potentially meaningful **interactions** among our variables, we used co-plots (i.e. conditioning plots) between some of our predictors and *effectiveness*, based on plausible hypotheses. However, to avoid overloading the present document, we only display some of the most informative plots. 

```{r erad coplot 1, fig.align='center', fig.cap="**Figure 2.11a.** Coplot of the *effectiveness score* displaying a three-way interaction between *elevation*, *fully_tarped* and *stand_surface* (in log-scale). There is a 5% overlap between each group, explaining why there are more than *n* = 85 points. Colours are meaningless and are just here for aesthetical reasons", message=FALSE, comment=NA, warning=FALSE}

par(.pardefault) # To restore defaults graphical parameters

### Three-way interaction between elevation, fully_tarped, and stand_surface
graphics::coplot(effectiveness ~ elevation | fully_tarped * log(stand_surface), data = erad, 
                 panel=function(x,y,...) { points(x,y, col = wesanderson::wes_palette(n=4, name="GrandBudapest1"), pch = 19); abline(line(y~x)) }, # The function in the `panel` argument is what enables the drawing of the regression lines! In the col argument, I simply created a palette of colours based on the movies of Wes Anderson (just to make the plot prettier).
                 number = 3, overlap = 0.05) # `number` indicates the number of groups by which the quantitative grouping variable (here, stand_surface) should be divided into. The `overlap` argument controls the proportion of points that are shared between the groups (here, 5%).
# Note that, sometimes coplot will create several rows for the same variable, but this can be controlled using the `rows` argument (e.g. rows = 1).
```

This coplot is very interesting because it shows an inconsistent pattern within each category (single row or single column) suggesting that our sample prevents us to satisfactorily test a three-way interaction between those variables. Additionally, we can see that there are very few observations of not entirely tarped knotweed stands (*n* = 17) as well as very few observations above 500m a.s.l. (*n* = 9; which is not a very high elevation for knotweeds (Martin *et al.*, 2019)). As our dataset was gathered opportunistically it is not surprising, yet it means that our sampled dataset will only be able to highlight strong effects of elevation and of partially tarped control operations, which is not the case here. All this may explain the very odd pattern displayed by the lower-right panel: Figure 2.2, Figure 2.6 and the other panels (save one) seem to indicate that there is a positive correlation between elevation and tarping effectiveness; however, the fact that this pattern is reversed for smaller knotweed stands (and thus weaker ones) that are entirely covered does not make any sense. It is therefore possible that the seemingly higher effectiveness of tarping at higher elevations actually reflects sampling stochasticity.

As we already have too many candidate predictors, we may consider dropping the *elevation* variable for further analyses. 

```{r erad coplot 2, fig.align='center', fig.cap="**Figure 2.11b.** Coplot of the *effectiveness score* displaying a two-way interaction between *fully_tarped* and *stand_surface* (in log-scale). There is a 5% overlap between each group, explaining why there are more than *n* = 85 points. Colours are meaningless and are just here for aesthetical reasons", message=FALSE, comment=NA, warning=FALSE}
### 
graphics::coplot(effectiveness ~ log(stand_surface) | fully_tarped, data = erad, 
                 panel=function(x,y,...) { points(x,y, col = wesanderson::wes_palette(n=4, name="GrandBudapest2"), pch = 19); abline(line(y~x)) }, 
                 number = 4, overlap = 0.05, rows = 1)
```

This above coplot seems to indicate that entirely covered (tarped) knotweed stands prevent the drop in tarping effectiveness with increasing stand surface. Caution is however still required as the relationships do not seem very strong.


```{r erad coplot 3, fig.align='center', fig.cap="**Figure 2.11c.** Coplot of the *effectiveness score* displaying a two-way interaction between *followups* and *pb_fixation*. There is a 5% overlap between each group, explaining why there are more than *n* = 85 points. Colours are meaningless and are just here for aesthetical reasons", message=FALSE, comment=NA, warning=FALSE}
graphics::coplot(effectiveness ~ followups | pb_fixation, data = erad, 
                 panel=function(x,y,...) { points(x,y, col = wesanderson::wes_palette(n=4, name="Royal2"), pch = 19); abline(line(y~x)) }, 
                 number = 2, overlap = 0.05, rows = 1)
```

Quite logically, this coplot seems to confirm that follow-up operations increase tarping effectiveness especially when there are some problems regarding fabrics fixation.

******

```{r erad export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "data" folder:
readr::write_csv(x = erad, file = here::here("data", "erad.csv"), append = FALSE)
# A problem with this method is that once exported, R will lose track of the variable types (class). Therefore, if we open (read) this .csv file with R, it will specify columns as best it can, that is as double for numeric columns and as character for columns containing letters. For instance, it will import the first column of `erad` (i.e. "manager_id") as a numeric (double) variable when it should be a factor.
# A not so ideal solution is to use the `col_types` argument of the readr::read_csv() function to manually specify variable types during import (see the readr vignette for more info).
```
\
\



# - Exploration of the sub-dataset related to the response variable indicating knotweed regrowths at the edge of the tarped area (i.e. *reg_edges*)
\
The first thing to do here was to prepare the sub-dataset that will be used to model the responses of the *reg_edges* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *reg_edges*;
* We left out the observations for which we could not obtain information on regrowths (i.e. NA);
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variable (computed as the Normalized Root Mean Squared Error (NRMSE) for numeric variables, and the Proportion of Falsely Classified entries (PFC) for categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation errors. For the meaning of the variables, please refer to the attached **documentation**.

```{r redges import & imput, include=FALSE}
redges <- model_datasets(response.var = "edges")
redges <- dplyr::mutate(.data = redges, 
                        reg_edges = as.factor(x = reg_edges),
                        reg_elsewhere = as.factor(x = reg_elsewhere),
                        uprootexcav = as.factor(x = uprootexcav))

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
redges %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> redges_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = redges_mis, verbose = TRUE)
```

```{r redges imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Here are a few descriptive statistics of our sub-dataset.

```{r redges summary, include=TRUE, message=FALSE, comment=NA}
redges[,3:ncol(redges)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(redges)

rm(redges_mis, imput)
```
\
\

## - Synthesis of contextual variables
\
We performed a **normed-PCA** on some of our environmental variables for which we did not have strong direct hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r redges env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 3.1.** Correlation plot of the normed-PCA on environmental variables for the *regrowth at the edges of the tarped area* dataset"}
xxx <- dplyr::select(.data = redges, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
redges %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> redges
redges$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable was named **coarse_env** and thus replaced the other 5 variables in the dataset. Operations with *positive values* for this variable were located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

We also performed another **normed-PCA** on some of our follow-up variables that we thought could be synthesized (i.e. *add_control*, *repairs*, and *freq_monitoring*).


```{r redges followups, warning=FALSE, fig.align='center', fig.cap="**Figure 3.1b.** Correlation plot of the normed-PCA on follow-up variables for the *tarping effectiveness* dataset"}
redges %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
redges %>% dplyr::rename(followups = "freq_monitoring") -> redges
redges$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their follow-up conditions. This new variable was named **followups** and thus replaced *freq_monitoring* in the dataset (we decided to keep *repairs* and *add_control* as is). High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\
  
## - Univariate distribution and dispersion parameters
### - Looking for outliers
\
The next step in our data exploration was to look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r redges boxplot outliers, fig.align='center', fig.cap="**Figure 3.2.** Boxplots for all quantitative variables of the *regrowth at the edges of the tarped area* dataset", warning=FALSE}
rm(res.pca, xxx, ttt)

uni.boxplots(dataset = redges)
```

```{r redges dotplot, fig.align='center', fig.cap="**Figure 3.3.** Cleveland dotplots for all quantitative variables of the *regrowth at the edges of the tarped area* dataset"}
uni.dotplots(dataset = redges)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *slope* variable looks fine.
* The *elevation*, *flood*, *obstacles*, *tarping_duration*, and *followups* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *distance*, *sedicover_height*, and *trench_depth* variables may require important transformations to be normalized. It would be interesting to see if their extreme values are too far from what could be expected from a Normal or a Poisson distribution (see below).
* The *latitude*, *longitude* and *stand_surface* variables contain problematic patterns/outliers.


```{r redges simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = redges, latitude, freq_monitoring, distance, sedicover_height, trench_depth)

uni.simudistrib(simu.var = simu.var, distribution = "normal")
uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)
```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we chose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^).    
The other variables do not have distribution that a transformation could not fix, except perhaps *latitude* and *longitude*. Yet, we do not intend on truly using these variables at this point (we included them in case we need to account for some spatial autocorrelation in the models). 


```{r redges update, include=FALSE}
redges <- redges[-(which.max(redges$stand_surface)),]
```
\
\
   
### - Normality
\
As explained before, we looked at skewness and kurtosis to gain better insight on our data. 

```{r redges histo, fig.align='center', fig.cap="**Figure 3.4.** Histograms for all quantitative variables of the *regrowth at the edges of the tarped area* dataset"}
uni.histograms(dataset = redges)

num.data <- redges[, sapply(redges, is.numeric)]
tab <- data.frame(moments::skewness(x = num.data), moments::kurtosis(x = num.data)-3)
knitr::kable(x = tab, digits = 3, col.names = c("Skewness", "Excess kurtosis"))
rm(num.data, tab)
```

From the above histograms and table, we can see that:
* Very few of our sampled variables are close to approximate a normal distribution. 
* Many variables are skewed to the right but in relatively reasonable proportions. 
* Some variables (e.g. *elevation*, *stand_surface*, *distance*, and *sedicover_height*) have skewness and/or kurtosis values possibly high enough to have undue influence on regression coefficients. These variables will perhaps require some transformations.
\
\

## - Multivariate relationships
### - Correlation among variables and multicollinearity
\
To investigate potential correlations and collinearity among our variables, we started by computing a **correlation matrix** of the data. As most of our factors were either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we could use them in a correlation matrix. To do that however, computations were based on *Spearman*'s rank correlation coefficients. 
To avoid giving too much weight to tarping experiments made by the same operators (that could be autocorrelated), we removed them from the data to compute correlations: their influence was accounted for in further analyses (only when modelling diagnostics indicated autocorrelation) by the use of **mixed-effect models**.

```{r redges corrplot, fig.align='center', fig.cap="**Figure 3.5.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *regrowth at the edges of the tarped area* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}

# To convert all ordered factors into numeric variables usable in a correlation matrix:
redges %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.redges # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.redges <- round(stats::cor(num.redges, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.redges <- ggcorrplot::cor_pmat(x = num.redges, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.redges, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.redges, insig = "blank")

rm(num.redges, res.cor.redges, res.pcor.redges)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* The *reg_edges* variable seems to be negatively correlated with *distance*, as we hypothesized. 
* *geomem* is extremely negatively correlated with *geotex*, which is logical as one is the opposite of the other. 
* Naturally, *followups* is strongly positively correlated with *repairs* and *add_control*. These latter two variables are also quite correlated with the variables related to *degradations*.
* *tarpfix_multimethod* seems to be strongly positively correlated with *trench_depth*, *tarpfix_pierced* and *plantation*, which is logical as trenches, staples and plantations are considered strong fabric fixation methods. 
* Without surprise, *degradation* seems to be strongly positively correlated with *pb_fixation* and *pb_durability*.
* *stand_surface* seems positively correlated with *flood*, *geotex*,*distance*, and *pb_fixation*.
* *obstacles* is positively correlated with *slope* and *coarse_env* (which are also positively correlated), as well as with *pb_durability* and *reg_elsewhere*.
* *followups* is positively correlated with *degradation* and *reg_elsewhere*.

At this stage and in accordance with the hierarchy of hypotheses we would like to test for the *reg_edges* variable, we decided to drop the following variables: *geotex* and *degradation*.

```{r redges corrdrop}
redges %>% dplyr::select(-geotex, -degradation) -> redges
```


\
\

### - Relationships among explanatory variables
\
To further study distributions and bivariate correlations, we decided to plot every numeric predictor against each others. It gives us the possibility to start investigating potential **multivariate outliers**.

```{r redges ggpairs, fig.align='center', fig.cap="**Figure 3.6.** Bivariate relationship among all numeric potential explanatory variables", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}

GGally::ggpairs(redges[, c("coarse_env", "distance", "flood", "followups", "obstacles", 
                         "stand_surface", "slope", "tarping_duration", "trench_depth")])
```

This figure confirms that most variables are far from being normally distributed but that's not necessarily a problem. If many scatterplots appear relatively compact and consistent, all highly skewed variables (e.g. *stand_surface*, *trench_depth* etc.) tend to produce what seem to be multivariate outliers that could become potentially problematic for modelling. In case it happens, transforming these variables may be useful.
\
\

### - Relationships between *reg_edges* and its potential explanatory variables
\
Since *reg_edges* will be modelled using logistic regressions, it is very important to ensure the respect of the **linearity assumption** between the outcome on the *logit scale* and each numeric predictor, for instance using Box-Tidwell tests or scatterplots. The main issue here is to get the values of the response variable on the logit scale. To do so, we have to build a logistic model incorporating all potential predictors to extract expected probabilities. A drawback from this approach is that the toy model can easily be misspecified in the process: for that reason, we actually checked this assumption iteratively for each *a priori* candidate model built for analyses (the scatterplots displayed below only serve to illustrate what transformations were actually applied to linearize relationships).

```{r redges linearity scatterplot, fig.align='center', fig.cap="**Figure 3.7.** Bivariate scatterplot between of the *reg_edges* variable on the *logit scale* and each potential numeric predictor", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
eff2 <- redges[,c("reg_edges", "distance", "followups", "obstacles", "slope", "stand_surface", "tarping_duration", "trench_depth")]
model <- glm(reg_edges ~., data = eff2, family = binomial)
# Predict the probability (p) of regrowths at the edge:
probabilities <- predict(model, type = "response")
# Transforming predictors
mydata <- eff2 %>%
  dplyr::select_if(is.numeric) %>%
  dplyr::mutate("stand_surface (log)" = log(stand_surface)) %>%
  dplyr::mutate("distance (log+1)" = log(distance + 1)) # These transformations were made to linearize the relationships
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot (ggplot2, so long format)
mydata <- mydata %>%
  dplyr::mutate(logit = log(probabilities/(1-probabilities))) %>%
  tidyr::gather(key = "predictors", value = "predictor.value", -logit)
# Create scatterplot
ggplot2::ggplot(mydata, ggplot2::aes(y = logit, x = predictor.value))+
  ggplot2::geom_point(size = 0.5, alpha = 0.5) +
  ggplot2::geom_smooth(method = "loess") +
  ggplot2::theme_bw() +
  ggplot2::facet_wrap(~predictors, scales = "free_x")
rm(eff2, model, probabilities, predictors, mydata)
```

This figure shows that log-transforming *distance* and *stand_surface* could help linearise their relationship with *reg_edges* expressed on the logit scale.

More general scatterplots can further be used to ensure the absence of bivariate **(quasi-)perfect separation** between a predictor and the response variable, which is highly problematic for logistic models fitted using Maximum Likelihood (Allison, 2004).

```{r redges bivariate plot, fig.align='center', fig.cap="**Figure 3.8.** Multivariate representation of the *reg_edges* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "reg_edges" variable (bounded continuous variable):
redges %>% dplyr::select(-manager_id, -xp_id, -latitude, -longitude) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(reg_edges), # Means that all columns of `mydata` need to be reshaped except "reg_edges" & "fully_tarped" (works as well with the form `!reg_edges`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = reg_edges)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,1)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "plum1") + 
  ggplot2::labs(x = "Predictor value", y = "Regrowth on the edges") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

There is no apparent sign of bivariate (quasi-)perfect separation, so regular logistic models can be used as well as multimodel inference.
\
\
   
### - Interactions among variables

Finally, to further investigate potentially meaningful **interactions** among our variables, we used co-plots (i.e. conditioning plots) between some of our predictors and *reg_edges*, based on plausible hypotheses. 
However, as *reg_edges* is a binary factor, the use of coplots was more complicated than for a numeric variable. Visualizing categorical variables as a function of other variables, especially quantitative ones, is not ideal in a regular conditioning plot as points will tend to be stacked on each other and as logistic curves should be fitted instead of regression lines, which is quite tedious to automatise in R. Consequently, we chose to draw *spineplots within the coplot panels* (**conditioning spineplots**) by transforming quantitative variables into two-level factors ("low" and "high" - based on whether observations had values below or above average, meaning that the number of observations is not distributed equitably between the two categories). The plots created could be viewed as some sort of *graphical contingency tables*. Unfortunately, combining spineplots with coplots creates a graphic bug in the drawing of the axes (linked to the coplot function internals) that we could not fix yet. We apologize for that.

To avoid overloading the present document, only the most informative plots are displayed. 

```{r redges coplot 1, fig.align='center', fig.cap="**Figure 3.9a.** Conditioning spineplot of *reg_edges* displaying a three-way interaction between *distance*, *stand_surface* (in log-scale) and *trench_depth*. Within each panel, the proportion of operations that found knotweed regrowths at the edge of the tarped area (dark orange) and the proportion of those that did not (light orange) is displayed while left-hand rectangles display these proportions for *short* tarping distances (below average) and right-hand rectangles display them for *long* tarping distances (above average)", message=FALSE, comment=NA, warning=FALSE}

par(.pardefault) # To restore defaults graphical parameters

redges %>% dplyr::mutate(distance = ifelse(distance <= mean(distance), "Short", "Long")) %>%
  dplyr::mutate(distance = factor(distance, levels=c("Short", "Long"))) -> rrr

pan.fun = function(x, y, ...){
    usr <- par("usr"); on.exit(par(usr)) # Frankly, I don't understand what these two lines do (only new = TRUE seems to be doing something), and the spineplot messes with the axes of the coplots below.
    par(usr = c(usr[1:2], 0, 1.5), new=T)
    spineplot(as.factor(x), as.factor(y), main="", xlab="", ylab="", axes=F, col = c("#FF9933","#FFCC99"))
}

### Three-way interaction between distance, stand_surface and trench_depth:
graphics::coplot(reg_edges ~ distance | log(stand_surface) * trench_depth, data = rrr, 
                 panel = pan.fun, number = 3, overlap = 0.01, rows = 1)
```

This conditioning spineplot is quite informative because it shows a changing pattern as we move from bottom to top, but globally not from left to right. It could be the sign of an existing interaction between *distance* and *trench depth*, but not with *stand surface*: with increasing trench depth, the proportion of tarping operations that found no knotweed regrowths at the edges (i.e. light orange rectangles) when tarping distances were above average (i.e. right-hand rectangles within each of the 9 panels) increased, regardless of the size of the knotweed stands. Actually, it is hard to say if stand surface did not play any role here but it appears that there was no unequivocal trend.


```{r redges coplot 2, fig.align='center', fig.cap="**Figure 3.9b.** Conditioning spineplot of *reg_edges* displaying a two-way interaction between *slope* and *obstacles*. Within each panel, the proportion of operations that found knotweed regrowths at the edge of the tarped area (dark orange) and the proportion of those that did not (light orange) is displayed while left-hand rectangles display these proportions for *gentle* slopes (below average) and right-hand rectangles display them for *steep* slopes (above average)", message=FALSE, comment=NA, warning=FALSE}

redges %>% dplyr::mutate(slope = ifelse(slope <= mean(slope), "Gentle", "Steep")) %>%
  dplyr::mutate(slope = factor(slope, levels=c("Gentle", "Steep"))) -> rrr

pan.fun = function(x, y, ...){
    usr <- par("usr"); on.exit(par(usr)) # Frankly, I don't understand what these two lines do (only new = TRUE seems to be doing something), and the spineplot messes with the axes of the coplots below.
    par(usr = c(usr[1:2], 0, 1.5), new=T)
    spineplot(as.factor(x), as.factor(y), main="", xlab="", ylab="", axes=F, col = c("#FF9933","#FFCC99"))
}

### Two-way interaction between slope and obstacles:
graphics::coplot(reg_edges ~ slope | obstacles, data = rrr, 
                 panel = pan.fun, number = 3, overlap = 0.01, rows = 1)
```

This above conditioning spineplot seems to indicate that the effect of increasing *slopes* on the proportion of regrowths at the edge increased when there were many *obstacles* on the tarped area.


```{r redges coplot 3, fig.align='center', fig.cap="**Figure 3.9c.** Conditioning spineplot of *reg_edges* displaying a three-way interaction between *obstacles*, *geomem* and *tarpfix_multimethod*. Within each panel, the proportion of operations that found knotweed regrowths at the edge of the tarped area (dark orange) and the proportion of those that did not (light orange) is displayed, while the left-hand rectangles display these proportions for *low* obstacle proportions (below average) and the right-hand rectangles display them for *high* obstacle proportions (above average)", message=FALSE, comment=NA, warning=FALSE}

redges %>% dplyr::mutate(obstacles = ifelse(obstacles <= mean(obstacles), "Low", "High")) %>%
  dplyr::mutate(obstacles = factor(obstacles, levels=c("Low", "High"))) -> rrr

pan.fun = function(x, y, ...){
    usr <- par("usr"); on.exit(par(usr)) # Frankly, I don't understand what these two lines do (only new = TRUE seems to be doing something), and the spineplot messes with the axes of the coplots below.
    par(usr = c(usr[1:2], 0, 1.5), new=T)
    spineplot(as.factor(x), as.factor(y), main="", xlab="", ylab="", axes=F, col = c("#FF9933","#FFCC99"))
}

### Three-way interaction between obstacles, geomem and tarpfix_multimethod:
graphics::coplot(reg_edges ~ obstacles | geomem * tarpfix_multimethod, data = rrr, 
                 panel = pan.fun, number = 3, overlap = 0.01, rows = 1)
```

This very interesting conditioning spineplot seems to indicate that when not much efforts had been taken to fix fabrics to the ground, increasing *obstacles* increased the likelihood of finding regrowths at the edge of the tarped area. However, when multiple methods were used for fixation, this trend seemed to be quite reduced except when *geomembranes* were used! This could mean that *geomembranes* are harder to properly fix to the ground and/or around obstacles (compared to *geotextiles*).


```{r redges coplot 4, fig.align='center', fig.cap="**Figure 3.9d.** Conditioning spineplot of *reg_edges* displaying a two-way interaction between *obstacles* and *distance*. Within each panel, the proportion of operations that found knotweed regrowths at the edge of the tarped area (dark orange) and the proportion of those that did not (light orange) is displayed while left-hand rectangles display these proportions for *low* obstacle proportions (below average) and right-hand rectangles display them for *high* obstacle proportions (above average)", message=FALSE, comment=NA, warning=FALSE}

### Two-way interaction between obstacles and obstacles:
graphics::coplot(reg_edges ~ obstacles | distance, data = rrr, 
                 panel = pan.fun, number = 3, overlap = 0.01, rows = 1)
```

This figure suggests that the effect of increasing *obstacle proportion* on the likelihood of finding regrowths at the edge of the tarped area might me be reduced when larger covering *distances* are applied.

******

```{r redges export, include=FALSE}
rm(rrr)
### Finally, I export the dataset to be used for modelling in my "data" folder:
readr::write_csv(x = redges, file = here::here("data", "redges.csv"), append = FALSE)
```
\
\




# - Exploration of the sub-dataset related to the response variable indicating knotweed regrowths at fabric strip overlaps (i.e. *reg_stripsoverlap*)
\
The first thing to do here was to prepare the sub-dataset that will be used to model the responses of the *reg_stripsoverlap* variable. To do so, the following steps have been performed:

* We selected the subset of predictors and covariates that will be used to model *reg_stripsoverlap*;
* We selected observations that had no missing values for *reg_stripsoverlap* and for *strips_overlap* as the first variable is our response value and the second is central for our hypotheses;
* Finally, we imputed missing values using **Random Forest** and the `missForest` package (Stekhoven & Buehlmann, 2012). 

The table below presents the *out-of-bag (OOB) imputation error* estimates for each type of variable (computed as the Normalized Root Mean Squared Error (NRMSE) for numeric variables, and the Proportion of Falsely Classified entries (PFC) for categorical variables). OOB values near 1 indicate that imputations are not reliable, while values close to 0 indicate very low imputation errors. For the meaning of the variables, please refer to the attached **documentation**.

```{r roverlaps import & imput, include=FALSE}
roverlaps <- model_datasets(response.var = "overlaps")
roverlaps <- dplyr::filter(.data = roverlaps, reg_stripsoverlap != "NA") %>%
  dplyr::filter(strips_overlap != "NA")

### Imputation of missing values using `missForest` (we could also have worked with `MICE`):
roverlaps %>% dplyr::select(-manager_id, -xp_id) %>% as.data.frame() -> roverlaps_mis # missForest only accepts data.frames or matrices (not tibbles). Variables must also be numeric or factors only (no character, date, difftime... classes)!

imput <- missForest::missForest(xmis = roverlaps_mis, verbose = TRUE)
```

```{r roverlaps imput error table, include=TRUE, message=FALSE, comment=NA}
knitr::kable(imput$OOBerror)
```

Here are a few descriptive statistics of our sub-dataset.

```{r roverlaps summary, include=TRUE, message=FALSE, comment=NA}
roverlaps[,3:ncol(roverlaps)] <- imput$ximp # Replacing the original dataset with the imputed one!
summary(roverlaps)

rm(roverlaps_mis, imput)
```
\
\

## - Synthesis of environmental variables
\
We performed a **normed-PCA** on some of our environmental variables for which we did not have strong direct hypotheses (i.e. *difficulty_access*, *shade*, *forest*, *ruggedness*, *granulometry*) but we kept *slope*, *obstacles*, and *flood* because we suspected they might have a major influence on the success/failure of tarping. 

```{r roverlaps env PCAs, warning=FALSE, fig.align='center', fig.cap="**Figure 4.1.** Correlation plot of the normed-PCA on environmental variables for the *regrowth at strips overlap* dataset"}
xxx <- dplyr::select(.data = roverlaps, c(difficulty_access, shade, forest, ruggedness, granulometry))

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
roverlaps %>% dplyr::select(-difficulty_access, -shade, -forest, -ruggedness) %>%
  dplyr::rename(coarse_env = "granulometry") -> roverlaps
roverlaps$coarse_env <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 5 environmental variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their environment. This new variable was named **coarse_env** and thus replaced the other 5 variables in the dataset. Operations with *positive values* for this variable were located in sites that tend to be forested (and thus shaded), difficult to access, with uneven ground and a coarse soil texture (and vice-versa).

We also performed another **normed-PCA** on some of our follow-up variables that we thought could be synthesized (i.e. *add_control*, *repairs*, and *freq_monitoring*).


```{r roverlaps followups, warning=FALSE, fig.align='center', fig.cap="**Figure 4.1b.** Correlation plot of the normed-PCA on follow-up variables for the *tarping effectiveness* dataset"}
roverlaps %>% dplyr::select(freq_monitoring, repairs, add_control) %>%
  dplyr::mutate(repairs = as.numfactor(x = repairs),
                add_control = as.numfactor(x = add_control))-> xxx

# Normed-PCA:
res.pca <- FactoMineR::PCA(X = xxx, scale.unit = TRUE, graph = FALSE)
# To get the correlation circle for this PCA, with a variable coloration according to their contribution to the first 2 principal components:
factoextra::fviz_pca_var(res.pca, col.var = "contrib",
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)

# As the first axis (PC) of my PCA satisfactorily synthesizes a fairly large amount of the variance of my 5 variables, I will use the coordinates of the tarping operations on this axis as a synthetic variable:
ttt <- res.pca$ind$coord[,1] 
roverlaps %>% dplyr::rename(followups = "freq_monitoring") -> roverlaps
roverlaps$followups <- ttt # And I use this occasion to reduce the dataset and replace one variable with my new synthetic variable.
```

As the first axis (PC) of our PCA satisfactorily synthesized a large amount (`r round(res.pca$eig[1,2],1)`%) of the variance of the 3 follow-up variables, we used operations' coordinates on this axis to build a synthetic variable to summarize their follow-up conditions. This new variable was named **followups** and thus replaced *freq_monitoring* in the dataset (we decided to keep *repairs* and *add_control* as is). High *positive values* for this variable represent operations that were frequently monitored and where additional control and repairs were performed (and vice-versa).
\
\
  
## - Univariate distribution and dispersion parameters
### - Looking for outliers
\
The next step in our data exploration was to look for potential outliers in the data using **boxplots** and **Cleveland dotplots** on our quantitative variables. 

```{r roverlaps boxplot outliers, fig.align='center', fig.cap="**Figure 4.2.** Boxplots for all quantitative variables of the *regrowth at strips overlap* dataset", warning=FALSE}
rm(res.pca, xxx, ttt)

uni.boxplots(dataset = roverlaps)
```

```{r roverlaps dotplot, fig.align='center', fig.cap="**Figure 4.3.** Cleveland dotplots for all quantitative variables of the *regrowth at strips overlap* dataset"}
uni.dotplots(dataset = roverlaps)
```

By the look of these graphs, it is clear that we have many *extreme values* (possible outliers). Yet, extreme values are not necessarily **true outliers**:

* The *slope* and *coarse_env* variables look fine.
* The *latitude*, *elevation*, *strips_overlap* and *tarping_duration* variables may require mild transformations to be normalized but do not seem to contain too extreme values.
* The *distance*, *sedicover_height*, *trench_depth*, and *followups* variables may require important transformations to be normalized. 
* The *latitude*, *longitude*, and *stand_surface* variables contain a problematic pattern/outlier.


```{r roverlaps simu.distrib, include=FALSE, eval=FALSE}
simu.var <- dplyr::select(.data = roverlaps, latitude, freq_monitoring, distance, sedicover_height, trench_depth)

uni.simudistrib(simu.var = simu.var, distribution = "normal")
uni.simudistrib(simu.var = simu.var, distribution = "log-normal")
uni.simudistrib(simu.var = simu.var, distribution = "poisson")
rm(simu.var)
```

After analysing random samples drawn from log-normal distributions (based on the parameters of our potentially problematic variables), we have good reason to believe that only *stand_surface* should be problematic. Consequently, we chose to delete the spotted outlier within *stand_surface* (i.e. the *stand_surface* of ca. 5000 m^2^). 
The other variables do not have distribution that a transformation could not fix, except perhaps *latitude* and *longitude*. Yet, we do not intend on truly using these variables at this point (we included them in case we need to account for some spatial autocorrelation in the models). 

```{r roverlaps update, include=FALSE}
roverlaps <- roverlaps[-(which.max(roverlaps$stand_surface)),]
```
\
\
   
### - Normality
\
As explained before, we looked at skewness and kurtosis to gain better insight on our data. 

```{r roverlaps histo, fig.align='center', fig.cap="**Figure 4.4.** Histograms for all quantitative variables of the *regrowth at strips overlap* dataset"}
uni.histograms(dataset = roverlaps)

num.data <- roverlaps[, sapply(roverlaps, is.numeric)]
tab <- data.frame(moments::skewness(x = num.data), moments::kurtosis(x = num.data)-3)
knitr::kable(x = tab, digits = 3, col.names = c("Skewness", "Excess kurtosis"))
rm(num.data, tab)
```

From the above histograms and table, we can see that:  
* Very few of our sampled variables are close to approximate a normal distribution. 
* Many variables are skewed to the right but in relatively reasonable proportions. 
* Some variables (e.g. *elevation*, *stand_surface*, *distance*, and *sedicover_height*) have skewness and/or kurtosis values possibly high enough to have undue influence on regression coefficients. These variables will perhaps require some transformations.
\
\

## - Multivariate relationships
### - Correlation among variables and multicollinearity
\
To investigate potential correlations and collinearity among our variables, we started by computing a **correlation matrix** of the data. As most of our factors were either ordered factors (i.e. ordinal variables) or binary factors (i.e. boolean), we could use them in a correlation matrix. To do that however, computations were based on *Spearman*'s rank correlation coefficients.  
To avoid giving too much weight to tarping experiments made by the same operators (that could be autocorrelated), we removed them from the data to compute correlations: their influence was accounted for in further analyses (only when modelling diagnostics indicated autocorrelation) by the use of **mixed-effect models**.

```{r roverlaps corrplot, fig.align='center', fig.cap="**Figure 4.5.** Correlation matrix displaying *Spearman*'s $\\rho$ for the numeric variables and ordered factors of the *regrowth at strips overlap* dataset. Only significant correlations are displayed (with $\\alpha$ = 0.05)", warning=FALSE}

# To convert all ordered factors into numeric variables usable in a correlation matrix:
roverlaps %>% dplyr::filter(!manager_id == 1) %>% # We remove our own experiments as they share many features that will strongly influence correlations (it should not be a problem for modelling as we will use mixed-effect models).
  dplyr::select(-manager_id, -xp_id) %>%
  dplyr::mutate_if(.predicate = is.factor, .funs = as.numfactor) -> num.roverlaps # as.numfactor() is my own custom function (see '01_data_cleaning.R').



# To compute the correlation matrix:
res.cor.roverlaps <- round(stats::cor(num.roverlaps, use = "complete.obs", method = "spearman"), 2)
# To compute a matrix of correlation p-values:
res.pcor.roverlaps <- ggcorrplot::cor_pmat(x = num.roverlaps, method = "spearman")

ggcorrplot::ggcorrplot(res.cor.roverlaps, type = "upper",
   outline.col = "white",
   ggtheme = ggplot2::theme_gray,
   colors = c("#6D9EC1", "white", "#E46726"), p.mat = res.pcor.roverlaps, insig = "blank")

rm(num.roverlaps, res.cor.roverlaps, res.pcor.roverlaps)
```

Many interesting patterns can be observed in this correlation matrix. Among them, we can see that:

* The *reg_stripsoverlap* variable seems to be mildly positively correlated with *obstacles*.
* As usual, *geomem* is extremely negatively correlated with *geotex*, which is logical since the use of geomembranes usually precludes the use of geotextiles (except in mixed settings).
* *stripfix_pierced* is strongly positively correlated with *tarpfix_pierced*, but also with *fully_tarped*.
* *stripfix_taped* is mildly positively correlated with *maxveg*, and negatively with *add_control*, and *reg_elsewhere*. 
* Conversely, *strips_overlap* is mildly negatively correlated with *pb_durability*. 
* *tarpfix_multimethod* seems to be positively correlated with *trench_depth* and *plantation*, which is logical.
* *levelling* is also correlated with *uprootexcav* and *trench_depth*, here again quite logically as the ground is often levelled after knotweed excavation or trench digging. 
* *obstacles* is, among other things, correlated with *coarse_env*, *distance*, *pb_durability* and *reg_elsewhere*.
* *reg_elsewhere* is also negatively correlated with *maxveg* and *trench_depth*, all these correlations seeming quite logical. 
* Finally, *stand_surface* seems positively correlated with *flood*, *geotex*, *plantation*, *tarpfix_pierced* and *pb_fixation*.

At this stage and in accordance with the hierarchy of hypotheses we would like to test for the *reg_stripsoverlap* variable, we decided to drop the following variables: *geotex*, *maxveg*, *uprootexcav* and *flood*.

```{r roverlaps corrdrop}
roverlaps %>% dplyr::select(-geotex, -maxveg, -uprootexcav, -flood) -> roverlaps
```


\
\

### - Relationships among explanatory variables
\
To further study distributions and bivariate correlations, we decided to plot every numeric predictor against each others. It gives us the possibility to start investigating potential **multivariate outliers**.

```{r roverlaps ggpairs, fig.align='center', fig.cap="**Figure 4.6.** Bivariate relationship among all numeric potential explanatory variables", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}

GGally::ggpairs(roverlaps[, c("coarse_env", "obstacles", "sedicover_height", "slope",
                         "stand_surface", "strips_overlap")])
```

This figure confirms that most variables are far from being normally distributed but that's not necessarily a problem. If many scatterplots appear relatively compact and consistent, all highly skewed variables (i.e. *stand_surface*, *sedicover_height*, and *strips_overlap*) tend to produce what seem to be multivariate outliers that could become potentially problematic for modelling. In case it happens, transforming these variables may be useful.
\
\

### - Relationships between *reg_stripsoverlap* and its potential explanatory variables
\
Since *reg_stripsoverlap* will be modelled using logistic regressions, it is very important to ensure the respect of the **linearity assumption** between the outcome on the *logit scale* and each numeric predictor, for instance using Box-Tidwell tests or scatterplots. The main issue here is to get the values of the response variable on the logit scale. To do so, we have to build a logistic model incorporating all potential predictors to extract expected probabilities. A drawback from this approach is that the toy model can easily be misspecified in the process: for that reason, we actually checked this assumption iteratively for each *a priori* candidate model built for analyses (the scatterplots displayed below only serve to illustrate what transformations were actually applied to linearize relationships).

```{r roverlaps linearity scatterplot, fig.align='center', fig.cap="**Figure 4.7.** Bivariate scatterplot between of the *reg_stripsoverlap* variable on the *logit scale* and each potential numeric predictor", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
eff2 <- roverlaps[,c("reg_stripsoverlap", "coarse_env", "obstacles", "sedicover_height", "slope", "stand_surface", "strips_overlap")]
model <- glm(reg_stripsoverlap ~., data = eff2, family = binomial)
# Predict the probability (p) of regrowths at strip overlaps:
probabilities <- predict(model, type = "response")
# Transforming predictors
mydata <- eff2 %>%
  dplyr::select_if(is.numeric) %>%
  dplyr::mutate("stand_surface (log)" = log(stand_surface)) %>%
  dplyr::mutate("sedicover_height (log+1)" = log(sedicover_height+1)) %>%
  dplyr::mutate("strips_overlap (sqrt)" = sqrt(strips_overlap)) # These transformations were made to linearize the relationships
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot (ggplot2, so long format)
mydata <- mydata %>%
  dplyr::mutate(logit = log(probabilities/(1-probabilities))) %>%
  tidyr::gather(key = "predictors", value = "predictor.value", -logit)
# Create scatterplot
ggplot2::ggplot(mydata, ggplot2::aes(y = logit, x = predictor.value))+
  ggplot2::geom_point(size = 0.5, alpha = 0.5) +
  ggplot2::geom_smooth(method = "loess") +
  ggplot2::theme_bw() +
  ggplot2::facet_wrap(~predictors, scales = "free_x")
rm(eff2, model, probabilities, predictors, mydata)
```

This figure shows that log-transforming *sedicover_height*, *stand_surface* and *strips_overlap* helped linearize their relationship with *reg_stripsoverlap* expressed on the logit scale.

More general scatterplots can further be used to ensure the absence of bivariate **(quasi-)perfect separation** between a predictor and the response variable, which is highly problematic for logistic models fitted using Maximum Likelihood (Allison, 2004).

```{r roverlaps bivariate plot, fig.align='center', fig.cap="**Figure 4.8.** Multivariate representation of *reg_stripsoverlap* against each potential explanatory variable", fig.width=12, fig.height=9, message=FALSE, comment=NA, warning=FALSE}
### To plot a response variable against all predictors/covariates, we will have to do use a for loop or the facet_wrap function of ggplot (or equivalent).
# For the "reg_stripsoverlap" variable (bounded continuous variable):
roverlaps %>% dplyr::select(-manager_id, -xp_id, -latitude, -longitude) %>%
  dplyr::mutate_if(is.factor, as.numfactor) %>% 
  tidyr::pivot_longer(-c(reg_stripsoverlap), # Means that all columns of `mydata` need to be reshaped except "reg_stripsoverlap" & "fully_tarped" (works as well with the form `!reg_stripsoverlap`, and we can place it somewhere else: actually, it is the `cols =` argument)! We have to work in the long-format because that's how the tidyverse and thus ggplot2 love to work...
                               names_to = "Predictor", # Gives the name of the new grouping variable while "values_to" gives the name of the variable that will be created from the data stored in the cell value.
                               values_to = "Value") %>% 
  ggplot2::ggplot(ggplot2::aes(x = Value, y = reg_stripsoverlap)) +
  ggplot2::facet_wrap(~Predictor, scales = "free_x", strip.position = "bottom") +
  ggplot2::geom_point(size = 1.5, alpha = 0.8, col = "palevioletred4") +
  ggplot2::scale_y_continuous(limits=c(0,1)) +
  ggplot2::geom_smooth(method = "lm", na.rm = TRUE, size = 1, col = "plum1") + 
  ggplot2::labs(x = "Predictor value", y = "Regrowth at strip overlaps") +
  ggthemes::theme_hc(style = "darkunica") +
  ggplot2::theme(panel.border = ggplot2::element_blank(),
          legend.key = ggplot2::element_blank(),
          strip.background = ggplot2::element_blank(),
          strip.text = ggplot2::element_text(colour = "gray60", size = 10),
          strip.placement = "outside",
          axis.line.x = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          axis.line.y = ggplot2::element_line(colour = "darkorange4", 
                                     size=0.5),
          legend.position = "right")
```

There is no apparent sign of bivariate (quasi-)perfect separation, so regular logistic models can be used as well as multimodel inference.
\
\
   
### - Interactions among variables

Finally and similarly as for other response variables, we would have liked to further investigate potentially meaningful **interactions** among our variables using co-plots (i.e. conditioning plots). Unfortunately, as *reg_stripsoverlap* contains only 22 successes (i.e. *n*~1~ = 22 and *n*~0~ = 52), our sample will not enable us to model interactions because it would require estimating three parameters, which is too many for such a small sample (Harrell, 2015).

******

```{r roverlaps export, include=FALSE}
### Finally, I export the dataset to be used for modelling in my "data" folder:
readr::write_csv(x = roverlaps, file = here::here("data", "roverlaps.csv"), append = FALSE)
```
\
\



# - References

* Allison P.D., 2004. *Convergence Problems in Logistic Regression*. In Altman M., Gill J. and McDonald M.P. (eds.), *Numerical Issues in Statistical Computing for the Social Scientist*. John Wiley & Sons, Hoboken: USA (pp. 238-252).
* Bolker B., 2007. *Ecological Models and Data in R*. Princeton University Press, Princeton: USA (408 p.).
* Field A., 2009. *Discovering statistics using SPSS*. SAGE Publications Ltd, London: The United Kingdom (952 p.).
* Fox J. and Monette G., 1992. Generalized Collinearity Diagnostics. *Journal of the American Statistical Association*, vol. 87: 178-183. doi: 10.2307/2290467
* Fox J., 2008. *Applied Regression Analysis and Generalized Linear Models, 2nd edition*. SAGE Publishing, Newbury Park: USA (688 p.).
* Gravetter F. and Wallnau,L., 2014. *Essentials of statistics for the behavioral sciences (8th ed.)*. Cengage Learning, Boston: USA (648 p.).
* Harrell F., 2015. *Regression Modeling Strategies. With Applications to Linear Models, Logistic and Ordinal Regression, and Survival Analysis*. Springer Nature, Cham: Switzerland (582 p.).
* Martin FM., Dommanget F., Janssen P. Spiegelberger T., Viguier C. and Evette A., 2019. Could knotweeds invade mountains in their introduced range? An analysis of patches dynamics along an elevational gradient. *Alpine Botany*, vol. 129: 33–42. doi: 10.1007/s00035-018-0214-5
* Montgomery D.C. and Peck E.A., 1992. *Introduction to Linear Regression Analysis*. John Wiley & Sons, Hoboken: USA (544 p.).
* Stekhoven DJ and Bühlmann P., 2012. MissForest--non-parametric missing value imputation for mixed-type data. *Bioinformatics*, vol. 28:112-118. doi: 10.1093/bioinformatics/btr597
* Zuur A.F., Ieno E.N. and Elphick C.S., 2010. A protocol for data exploration to avoid common statistical problems. *Methods in Ecology and Evolution*, vol. 1: 3-14. doi: 10.1111/j.2041-210X.2009.00001.x
\
